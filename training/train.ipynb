{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.410645\n",
      "Epoch: 2 \tTraining Loss: 0.141948\n",
      "Epoch: 3 \tTraining Loss: 0.092934\n",
      "Epoch: 4 \tTraining Loss: 0.070609\n",
      "Epoch: 5 \tTraining Loss: 0.057050\n",
      "Epoch: 6 \tTraining Loss: 0.046860\n",
      "Epoch: 7 \tTraining Loss: 0.041463\n",
      "Epoch: 8 \tTraining Loss: 0.035764\n",
      "Epoch: 9 \tTraining Loss: 0.032735\n",
      "Epoch: 10 \tTraining Loss: 0.027682\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "torch.manual_seed(6666)\n",
    "\n",
    "train_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, output=10):\n",
    "        super(ConvNet, self).__init__()        \n",
    "        self.conv1 = torch.nn.Conv2d(1, 4, kernel_size=7, padding=0, stride=3)\n",
    "        self.flatten = torch.nn.Flatten(1)\n",
    "        self.fc1 = torch.nn.Linear(256, 100)\n",
    "        self.fc2 = torch.nn.Linear(100, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x * x\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = x * x\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, n_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = ConvNet()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model = train(model, train_loader, criterion, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.091254\n",
      "\n",
      "Test Accuracy of 0: 98% (969/980)\n",
      "Test Accuracy of 1: 99% (1128/1135)\n",
      "Test Accuracy of 2: 98% (1020/1032)\n",
      "Test Accuracy of 3: 97% (982/1010)\n",
      "Test Accuracy of 4: 99% (975/982)\n",
      "Test Accuracy of 5: 97% (873/892)\n",
      "Test Accuracy of 6: 98% (939/958)\n",
      "Test Accuracy of 7: 97% (998/1028)\n",
      "Test Accuracy of 8: 97% (947/974)\n",
      "Test Accuracy of 9: 97% (980/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9811/10000)\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, pred = torch.max(output, 1)\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        for i in range(len(target)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    test_loss = test_loss/len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
    "\n",
    "    for label in range(10):\n",
    "        print(\n",
    "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
    "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% ' \n",
    "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
    "    )\n",
    "    \n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, f in enumerate(model.conv1.weight.detach().numpy()):\n",
    "    np.savetxt(f\"CW{i}.csv\", f[0], delimiter=\",\")\n",
    "np.savetxt(f\"CB.csv\", model.conv1.bias.detach().numpy(), delimiter=\",\")\n",
    "\n",
    "np.savetxt(f\"LW1.csv\", model.fc1.weight.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt(f\"LB1.csv\", model.fc1.bias.detach().numpy(), delimiter=\",\")\n",
    "\n",
    "np.savetxt(f\"LW2.csv\", model.fc2.weight.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt(f\"LB2.csv\", model.fc2.bias.detach().numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "class MahaDist(torch.nn.Module):\n",
    "    def __init__(self, output=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 4, kernel_size=7, padding=0, stride=3)\n",
    "        self.flatten = torch.nn.Flatten(1)\n",
    "        self.fc1 = torch.nn.Linear(256, 100)\n",
    "        self.fc2 = torch.nn.Linear(100, output)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "maha = MahaDist()\n",
    "model.conv1 = model.conv1\n",
    "maha.conv1.bias = model.conv1.bias\n",
    "maha.fc1.weight = model.fc1.weight\n",
    "maha.fc1.bias = model.fc1.bias\n",
    "maha.fc2.weight = model.fc2.weight\n",
    "maha.fc2.bias = model.fc2.bias\n",
    "\n",
    "@torch.no_grad()\n",
    "def fit_mahalanobis(maha, loader, num_classes=10):\n",
    "    maha.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for x, y in loader:\n",
    "        f = maha.forward_features(x).numpy()\n",
    "        features.append(f)\n",
    "        labels.append(y.numpy())\n",
    "\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    # class means\n",
    "    means = []\n",
    "    for c in range(num_classes):\n",
    "        means.append(features[labels == c].mean(axis=0))\n",
    "    means = np.stack(means, axis=0)\n",
    "\n",
    "    lw = LedoitWolf().fit(features)     # Σ estimate\n",
    "    precision = lw.precision_        # Σ^{-1}\n",
    "    return means, precision\n",
    "\n",
    "def mahalanobis_dist_diag(features, means, precision_diag):\n",
    "    diff = features[:, None, :] - means[None, :, :]\n",
    "    dists = ((diff * diff) * precision_diag[None, None, :]).sum(axis=2)\n",
    "    return -dists.min(axis=1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_dists_diag(maha, loader, means, precision_diag):\n",
    "    maha.eval()\n",
    "    dists = []\n",
    "    ys = []\n",
    "    for x, y in loader:\n",
    "        f = maha.forward_features(x).detach().cpu().numpy()\n",
    "        s = mahalanobis_dist_diag(f, means, precision_diag)\n",
    "        dists.append(s)\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "    return np.concatenate(dists), np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR on OOD = 0.013458662678914763\n",
      "TPR on ID = 0.9556\n",
      "Min OOD dist = -234068.04295735466\n",
      "AUROC = 0.9971\n",
      "AUPR  = 0.9935\n",
      "tau = -7324.3396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "ood_data = datasets.ImageFolder(root=\"data/notMNIST_small\",\n",
    "                                transform=transforms.Compose([\n",
    "                                        transforms.Grayscale(num_output_channels=1),\n",
    "                                        transforms.Resize((28, 28)),\n",
    "                                        transforms.ToTensor(),\n",
    "]))\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "means, precision = fit_mahalanobis(maha, train_loader, num_classes=10)\n",
    "precision_diag = np.diag(precision).copy()\n",
    "precision_diag = np.maximum(precision_diag, 1e-12)\n",
    "train_dists, _ = get_dists_diag(maha, train_loader, means, precision_diag)\n",
    "test_dists,  _ = get_dists_diag(maha, test_loader,  means, precision_diag)\n",
    "ood_dists,   _ = get_dists_diag(maha, ood_loader,   means, precision_diag)\n",
    "tau = np.quantile(train_dists, 0.05) # threshold at 95% TPR on ID (i.e., accept 95% of ID)\n",
    "\n",
    "# predicted as OOD if dist < tau\n",
    "fpr = (ood_dists >= tau).mean()    # OOD incorrectly accepted as ID\n",
    "tpr = (test_dists >= tau).mean()    # ID correctly accepted as ID\n",
    "print(\"FPR on OOD =\", fpr)\n",
    "print(\"TPR on ID =\",tpr)\n",
    "print(\"Min OOD dist =\",ood_dists.min())\n",
    "\n",
    "y_true = np.concatenate([\n",
    "    np.ones_like(test_dists),\n",
    "    np.zeros_like(ood_dists)\n",
    "])\n",
    "\n",
    "y_dist = np.concatenate([\n",
    "    test_dists,\n",
    "    ood_dists\n",
    "])\n",
    "\n",
    "auroc = roc_auc_score(y_true, y_dist)\n",
    "aupr  = average_precision_score(y_true, y_dist)\n",
    "\n",
    "print(f\"AUROC = {auroc:.4f}\")\n",
    "print(f\"AUPR  = {aupr:.4f}\")\n",
    "print(f\"tau = {tau:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f\"Means.csv\", means, delimiter=\",\")\n",
    "np.savetxt(f\"Precision.csv\", precision_diag, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calib] train baseline on FULL train set...\n",
      "[calib] 10 trials remove-one, eval N=3200 test samples (max_batches=50)\n",
      "  [t=1/10] ridx=38163 meanΔ=12.29\n",
      "  [t=2/10] ridx=7875 meanΔ=10.75\n",
      "  [t=3/10] ridx=19170 meanΔ=10.68\n",
      "  [t=4/10] ridx=5466 meanΔ=11.45\n",
      "  [t=5/10] ridx=12132 meanΔ=11.88\n",
      "  [t=6/10] ridx=43950 meanΔ=10.73\n",
      "  [t=7/10] ridx=51948 meanΔ=10.9\n",
      "  [t=8/10] ridx=4263 meanΔ=9.847\n",
      "  [t=9/10] ridx=57318 meanΔ=9.472\n",
      "  [t=10/10] ridx=9135 meanΔ=9.704\n",
      "Test subset size N = 3200 (max_batches=50)\n",
      "Quantile q=0.99: Delta_wp(q) = 35.4959\n",
      "DP params: (eps=16.0, delta=1e-05)  factor=4.84481\n",
      "sigma_wp = 10.7482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Delta_q': 35.49586045160214,\n",
       " 'sigma_wp': 10.748158219789284,\n",
       " 'eps': 16.0,\n",
       " 'delta': 1e-05,\n",
       " 'q': 0.99,\n",
       " 'M': 10,\n",
       " 'test_N': 3200,\n",
       " 'test_max_batches': 50,\n",
       " 'train_epochs': 10,\n",
       " 'lr': 0.001,\n",
       " 'seed': 6666}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "def g_gaussian(delta: float) -> float:\n",
    "    return math.sqrt(2.0 * math.log(1.25 / delta))\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_logits(model, loader, device=\"cpu\", max_batches=None):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for b, (x, _) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        outs.append(model(x).detach().cpu())\n",
    "        if max_batches is not None and (b + 1) >= max_batches:\n",
    "            break\n",
    "    return torch.cat(outs, dim=0)\n",
    "\n",
    "def train_one(model_ctor, train_ds, batch_size=64, n_epochs=10, lr=1e-3, seed0=6666, device=\"cpu\"):\n",
    "    torch.manual_seed(seed0)\n",
    "    np.random.seed(seed0)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    m = model_ctor().to(device)\n",
    "    crit = torch.nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "\n",
    "    m.train()\n",
    "    for _ in range(n_epochs):\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device); y = y.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(m(x), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    m.eval()\n",
    "    return m\n",
    "\n",
    "def remove_one(train_ds, ridx: int):\n",
    "    idxs = list(range(len(train_ds)))\n",
    "    idxs.pop(int(ridx))\n",
    "    return torch.utils.data.Subset(train_ds, idxs)\n",
    "\n",
    "def calibrate_sigma_wp(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    model_ctor,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    lr=1e-3,\n",
    "    device=\"cpu\",\n",
    "    seed0=6666,\n",
    "    M=10,                  # number of remove-one trials\n",
    "    max_batches=50,        # number of test batches to use (None = full test)\n",
    "    q=0.99,                # Delta quantile\n",
    "    eps=16.0,\n",
    "    delta=1e-5,\n",
    "):\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"[calib] train baseline on FULL train set...\")\n",
    "    base_model = train_one(model_ctor, train_data, batch_size, n_epochs, lr, seed0, device)\n",
    "    base_logits = collect_logits(base_model, test_loader, device=device, max_batches=max_batches)\n",
    "    N = base_logits.shape[0]\n",
    "\n",
    "    rng = np.random.default_rng(seed0)\n",
    "    remove_idxs = rng.choice(len(train_data), size=M, replace=False)\n",
    "\n",
    "    all_l2 = []\n",
    "    print(f\"[calib] {M} trials remove-one, eval N={N} test samples (max_batches={max_batches})\")\n",
    "    for t, ridx in enumerate(remove_idxs, start=1):\n",
    "        adj_ds = remove_one(train_data, ridx)\n",
    "        adj_model = train_one(model_ctor, adj_ds, batch_size, n_epochs, lr, seed0, device)\n",
    "        adj_logits = collect_logits(adj_model, test_loader, device=device, max_batches=max_batches)\n",
    "        diff = base_logits - adj_logits\n",
    "        l2 = torch.linalg.vector_norm(diff, ord=2, dim=1).numpy()\n",
    "        all_l2.append(l2)\n",
    "        print(f\"  [t={t}/{M}] ridx={int(ridx)} meanΔ={l2.mean():.4g}\")\n",
    "\n",
    "    all_l2 = np.concatenate(all_l2, axis=0)\n",
    "    Delta_q = float(np.quantile(all_l2, q))\n",
    "\n",
    "    factor = g_gaussian(delta)\n",
    "    sigma_wp = (Delta_q / eps) * factor\n",
    "\n",
    "    print(f\"Test subset size N = {N} (max_batches={max_batches})\")\n",
    "    print(f\"Quantile q={q}: Delta_wp(q) = {Delta_q:.6g}\")\n",
    "    print(f\"DP params: (eps={eps}, delta={delta})  factor={factor:.6g}\")\n",
    "    print(f\"sigma_wp = {sigma_wp:.6g}\")\n",
    "\n",
    "    return {\n",
    "        \"Delta_q\": Delta_q,\n",
    "        \"sigma_wp\": sigma_wp,\n",
    "        \"eps\": eps,\n",
    "        \"delta\": delta,\n",
    "        \"q\": q,\n",
    "        \"M\": M,\n",
    "        \"test_N\": int(N),\n",
    "        \"test_max_batches\": max_batches,\n",
    "        \"train_epochs\": n_epochs,\n",
    "        \"lr\": lr,\n",
    "        \"seed\": seed0,\n",
    "    }\n",
    "\n",
    "calibrate_sigma_wp(train_data=train_data, test_data=test_data, model_ctor=lambda: ConvNet())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
